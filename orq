#!/usr/bin/python
# vim: sw=2 et

"""
usage:
  orq run my_app.orq

  builds new docker image on dev machine.
  push to registry on host over ssh tunnel.
  start new container on host, redirect hipache, take down old container.

  orq is a process monitor, container manger, and proxy configurer.
  security all based on ssh tunnels.

task json schema:

  basics:
  [{
    "name": "my_app",
    "dockerdir": ".",
    "domain": "myapp.com",
    "http_port": 3000,
    "host": "box-1.myapp.com",
  }]

  other allowed keys:

    "volumes": [
      {"id": "app_logs", "container": "/app/logs"}
      ],

    note that volumes are placed in /var/lib/orq/volumes/<id>

  for internal use only:

    "raw_image_name": "something/else",
    "exposed_ports": [{"host": 5000, "container": 5000, "public": ...}],

"""

import os, sys, json, socket, SocketServer, threading, subprocess, time
import tempfile, shutil
import redis

IS_ROOT = os.getuid() == 0

ORQ_PORT = 4999
REG_PORT = 5000
if IS_ROOT:
  PUBLIC_PORT = 80
  ORQ_ROOT = '/var/lib/orq'
else:
  PUBLIC_PORT = 8000
  ORQ_ROOT = '/tmp/orq'
STATE_FILE = os.path.join(ORQ_ROOT, 'state')
VOL_ROOT = os.path.join(ORQ_ROOT, 'volumes')
LOCALHOST = '127.0.0.1'
ORQ_ADDR = (LOCALHOST, ORQ_PORT)
CHECK_INTERVAL = 10

HIPACHE_TASK_NAME = '__hipache__'
HIPACHE_IMAGE_NAME = 'orq_hipache'
HIPACHE_TASK = {
    'name': HIPACHE_TASK_NAME,
    'raw_image_name': HIPACHE_IMAGE_NAME,
    'volumes': [
      {'id': 'hipache_logs', 'container': '/var/log/hipache'}
      ],
    'exposed_ports': [
      {'host': PUBLIC_PORT, 'container': 8000, 'public': True},
      ],
    }

REGISTRY_TASK_NAME = '__registry__'
REGISTRY_TASK = {
    'name': REGISTRY_TASK_NAME,
    'raw_image_name': 'stackbrew/registry',
    'volumes': [
      {'id': 'docker_registry_data', 'container': '/tmp/registry'}
      ],
    'exposed_ports': [
      {'host': REG_PORT, 'container': REG_PORT},
      ],
    }

HIPACHE_IMAGE_SOURCES = {

'Dockerfile': r'''
FROM stackbrew/ubuntu:raring

# From https://gist.github.com/jpetazzo/6127116:
RUN echo "force-unsafe-io" > /etc/dpkg/dpkg.cfg.d/02apt-speedup
RUN echo "Acquire::http {No-Cache=True;};" > /etc/apt/apt.conf.d/no-cache

RUN apt-get -yq update
RUN apt-get -yq upgrade
RUN apt-get -yq install wget git python2.7-minimal redis-server supervisor
RUN apt-get -yq clean
RUN ln -sf python2.7 /usr/bin/python
RUN useradd -U -M -u 1000 hipache

RUN wget -qO- http://nodejs.org/dist/v0.8.26/node-v0.8.26-linux-x64.tar.gz | tar -C /usr/local/ --strip-components=1 -zx
RUN npm install -g hipache
RUN mkdir -p /etc/hipache
ADD ./supervisord.conf /etc/supervisor/supervisord.conf
ADD ./config.json /etc/hipache/config.json
ADD ./redis.conf /etc/redis/redis.conf
USER hipache
EXPOSE 8000
VOLUME ["/var/log/hipache"]
CMD ["supervisord", "-n"]
''',

'config.json': r'''
{
  "server": {
    "accessLog": "/var/log/hipache/access.log",
    "port": 8000,
    "workers": 5,
    "maxSockets": 1000,
    "deadBackendTTL": 30,
    "tcpTimeout": 30,
    "retryOnError": 3,
    "deadBackendOn500": true,
    "httpKeepAlive": false
  }
}
''',

'redis.conf': r'''
daemonize no
port 6379
# unixsocket /var/run/redis/redis.sock
# unixsocketperm 755
timeout 0
loglevel notice
logfile /var/log/hipache/redis.log
databases 1
#save 900 1
#save 300 10
#save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /tmp
slave-serve-stale-data yes
slave-read-only yes
slave-priority 100
appendonly no
no-appendfsync-on-rewrite yes
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
lua-time-limit 5000
slowlog-log-slower-than 10000
slowlog-max-len 128
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-entries 512
list-max-ziplist-value 64
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
activerehashing yes
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
''',

'supervisord.conf': r'''
[supervisord]
nodaemon=true
logfile=/var/log/hipache/supervisord.log
pidfile=/tmp/supervisord.pid
childlogdir=/var/log/hipache

[program:hipache]
command=/usr/local/bin/hipache -c /etc/hipache/config.json
autorestart=true

[program:redis]
command=/usr/bin/redis-server /etc/redis/redis.conf
autorestart=true
''',

}


def volume_path(vid):
  return os.path.join(VOL_ROOT, vid)

def docker(*argv):
  argv = list(argv)
  argv[0:0] = ['docker']
  print '+', ' '.join(argv)
  return subprocess.check_output(argv)

def inspect(thing, *keys):
  data = json.loads(docker('inspect', thing))
  data = data[0]
  for k in keys:
    data = data[k]
  return data

def ipaddr_of(cid):
  return inspect(cid, 'NetworkSettings', 'IPAddress')

def image_in_registry(image):
  if '/' in image:
    return image
  else:
    return 'localhost:%d/%s' % (REG_PORT, image)

def do_client(data):
  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
  s.connect(ORQ_ADDR)
  s.sendall(json.dumps(data))
  s.shutdown(socket.SHUT_WR)
  return json.load(s.makefile('r'))

def print_json(out):
  json.dump(out, sys.stdout, sort_keys=True,
      indent=4, separators=(',', ': '))
  print

def get_public_ip(cache=[]):
  if not cache:
    cmd = r"ifconfig eth0 | sed -ne 's/.* inet addr:\([0-9.]*\).*/\1/p'"
    output = subprocess.check_output(cmd, shell=True)
    cache.append(output.strip())
  return cache[0]


class ServerState(object):
  """
  schema:
  data = {
    'version': 1,
    'tasks': [
      {
        'task': <task json>,
        'cid': 'abc123', # or None
      },
    ],
  }
  """
  def find_tdata_by_name(self, name):
    for tdata in self.data['tasks']:
      if tdata['task']['name'] == name:
        return tdata

  def add_task(self, task):
    assert self.find_tdata_by_name(task['name']) is None
    tdata = {'task': task, 'cid': None}
    self.data['tasks'].append(tdata)
    return tdata

  def __enter__(self):
    try:
      self.data = json.load(open(STATE_FILE, 'r'))
      # TODO: check version
    except IOError:
      self.data = {'version': 1, 'tasks': []}
      self.add_task(HIPACHE_TASK)
      self.add_task(REGISTRY_TASK)
    return self

  def save(self):
    json.dump(self.data, open(STATE_FILE + '.tmp', 'w'))
    os.rename(STATE_FILE + '.tmp', STATE_FILE)

  def __exit__(self, t, v, t2):
    self.save()


class ServerHandler(SocketServer.StreamRequestHandler):
  def handle(self):
    try:
      data = json.load(self.rfile)
      print '>', data # debug
      result = getattr(self, 'op_' + data['op'])(data)
      out = {'success': True, 'result': result}
      print '<', out # debug
      json.dump(out, self.wfile)
    except:
      out = {'success': False}
      json.dump(out, self.wfile)
      raise  # FIXME

  @staticmethod
  def construct_docker_args(task):
    args = []

    args.append('-d')

    for vol in task.get('volumes', []):
      args.append('-v')
      host = volume_path(vol['id'])
      args.append('%s:%s' % (host, vol['container']))

    for port in task.get('exposed_ports', []):
      host = port['host']
      container = port['container']
      if port.get('public'):
        expose_ip = get_public_ip()
      else:
        expose_ip = LOCALHOST
      args.append('-p')
      args.append('%s:%s:%s' % (expose_ip, host, container))

    for k, v in task.get('env', {}).items():
      args.append('-e')
      args.append('%s=%s' % (k, v))

    if task.get('raw_image_name'):
      args.append(task['raw_image_name'])
    else:
      args.append(image_in_registry(task['name']))

    args.extend(task.get('argv', []))

    return args

  @staticmethod
  def ensure_host_volumes(task):
    for vol in task.get('volumes', []):
      path = volume_path(vol['id'])
      if not os.path.isdir(path):
        os.makedirs(path)
        os.chmod(path, 01777)

  @staticmethod
  def hipache_redis(st):
    hipache_cid = st.find_tdata_by_name(HIPACHE_TASK_NAME)['cid']
    hipache_ip = ipaddr_of(hipache_cid)
    return redis.StrictRedis(hipache_ip)

  @staticmethod
  def set_hipache_backend(st, domain, ip, port):
    r = ServerHandler.hipache_redis(st)

    endpoint = 'http://%s:%d' % (ip, port)
    key = 'frontend:%s' % domain

    # TODO: simplify and wrap in a transaction
    existing_keys = r.llen(key)
    if existing_keys == 0:
      r.rpush(key, domain)
    r.linsert(key, 'after', domain, endpoint)
    while r.llen(key) > 2:
      r.rpop(key)

  @staticmethod
  def clear_hipache_backend(st, domain):
    r = ServerHandler.hipache_redis(st)
    r.delete('frontend:%s' % domain)

  def op_check_running(self, data):
    with ServerState() as st:
      for task in st.data['tasks']:
        cid = task['cid']
        try:
          if cid and inspect(cid, 'State', 'Running'):
            continue
        except subprocess.CalledProcessError:
          pass
        self.run_task(st, task['task'])
        st.save()

  def run_task(self, st, task):
    self.ensure_host_volumes(task)

    # add to state and get old cid
    tdata = st.find_tdata_by_name(task['name'])
    if not tdata:
      tdata = st.add_task(task)
    tdata['task'] = task
    old_cid = tdata['old_cid'] = tdata['cid']

    # run new thing
    args = self.construct_docker_args(task)
    new_cid = tdata['cid'] = docker('run', *args).strip()

    time.sleep(task.get('up_wait_time', 1))

    # move hipache backend
    domain = task.get('domain')
    port = task.get('http_port')
    if domain and port:
      ip = ipaddr_of(new_cid)
      self.set_hipache_backend(st, domain, ip, port)

    # take down old thing
    if old_cid:
      time.sleep(task.get('down_wait_time', 1))
      docker('stop', old_cid)

  def op_ping(self, data):
    return None

  def op_run(self, data):
    task = data['task']
    with ServerState() as st:
      self.run_task(st, task)

  def op_stop(self, data):
    name = data['task']['name']  # only pull out name, ignore rest
    with ServerState() as st:
      tdata = st.find_tdata_by_name(name)
      if tdata:
        task = tdata['task']
        cid = tdata['cid']
        if 'domain' in task:
          self.clear_hipache_backend(st, task['domain'])
        if cid:
          docker('stop', cid)
        st.data['tasks'].remove(tdata)

  def op_cleanup(self, data):
    subprocess.call('''
      # delete all non-running container
      docker ps -a | awk '/Exit/ {print $1}' | xargs -r docker rm
      # delete unused images
      docker images | awk '/<none>/ {print $3}' | xargs -r docker rmi
    ''', shell=True)


def build_hipache():
  tmpdir = tempfile.mkdtemp()
  os.chdir(tmpdir)
  for k, v in HIPACHE_IMAGE_SOURCES.items():
    open(k, 'w').write(v)
  docker('build', '-rm', '-t', HIPACHE_IMAGE_NAME, '.')
  os.chdir('/')
  shutil.rmtree(tmpdir)


def run_daemon():
  # build hipache image
  try:
    inspect(HIPACHE_IMAGE_NAME, 'container_config')
  except (ValueError, subprocess.CalledProcessError):
    build_hipache()

  # set up for daemon
  os.umask(077)
  for d in ORQ_ROOT, VOL_ROOT:
    if not os.path.isdir(d):
      os.makedirs(d)
  os.chdir(ORQ_ROOT)

  def send_check_requests():
    while True:
      time.sleep(CHECK_INTERVAL)
      try: do_client({'op': 'check_running'})
      except: pass
  scr = threading.Thread(target=send_check_requests)
  scr.setDaemon(True)
  scr.start()

  SocketServer.TCPServer(ORQ_ADDR, ServerHandler).serve_forever()


class SshTunnel(object):
  def __init__(self, host):
    self.host = host

  def __enter__(self):
    args = [
        'ssh',
        self.host,
        '-L', 'localhost:%d:127.0.0.1:%d' % (ORQ_PORT, ORQ_PORT),
        '-L', 'localhost:%d:127.0.0.1:%d' % (REG_PORT, REG_PORT),
        'sleep 1d',
        ]
    self.proc = subprocess.Popen(args,
        close_fds=True,
        stdin=open('/dev/null', 'r'),
        stdout=open('/dev/null', 'w'))
    while True:
      try:
        do_client({'op': 'ping'})
        break
      except socket.error:
        time.sleep(1)

  def __exit__(self, t, v, t2):
    self.proc.terminate()
    self.proc.wait()


class Cli(object):
  def handle(self, argv):
    return getattr(self, 'cmd_' + argv[0])(argv[1:])

  @staticmethod
  def load_taskfile(taskfile):
    return json.load(open(taskfile, 'r'))

  def cmd_daemon(self, argv):
    run_daemon()

  def cmd_run(self, argv):
    tasks = self.load_taskfile(argv[0])
    # TODO: group by host so we don't do extra ssh work
    for task in tasks:
      dockerdir = os.path.join(os.path.dirname(argv[0]), task['dockerdir'])
      host = task['host']
      image = image_in_registry(task['name'])
      with SshTunnel(host):
        docker('build', '-t', image, dockerdir)
        docker('push', image)
        print_json(do_client({'op': 'run', 'task': task}))

  def cmd_stop(self, argv):
    tasks = self.load_taskfile(argv[0])
    for task in tasks:
      host = task['host']
      with SshTunnel(host):
        print_json(do_client({'op': 'stop', 'task': task}))

  def cmd_cleanup(self, argv):
    with SshTunnel(argv[0]):
      print_json(do_client({'op': 'cleanup'}))


if __name__ == '__main__':
  sys.exit(Cli().handle(sys.argv[1:]))
